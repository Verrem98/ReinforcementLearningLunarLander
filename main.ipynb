{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from collections import deque\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Transition:\n",
    "    def __init__(self, state, action, reward, next_state, terminated):\n",
    "        self.state = state\n",
    "        self.action = action\n",
    "        self.reward = reward\n",
    "        self.next_state = next_state\n",
    "        self.terminated = terminated\n",
    "        \n",
    "        self.data = (self.state, self.action, self.reward, self.next_state, self.terminated)\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f\"Transition{self.data}\"\n",
    "    def __iter__(self):\n",
    "        return iter(self.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Memory:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = deque(maxlen=capacity)\n",
    "\n",
    "    def store(self, transition):\n",
    "        self.memory.append(transition)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.memory, batch_size)\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, policy, memory):\n",
    "\n",
    "        self.policy = policy\n",
    "        self.memory = memory\n",
    "\n",
    "    def select_action(self, state):\n",
    "        return self.policy.select_action(state)\n",
    "\n",
    "    def decay(self):\n",
    "        self.policy.epsilon *= self.policy.epsilon_decay_rate\n",
    "        \n",
    "    def train(self, batch_size):\n",
    "        \n",
    "        # zorg ervoor dat we niet crashen omdat er niet genoeg Transitions in de memory zitten!\n",
    "        if batch_size > len(self.memory.memory):\n",
    "            return\n",
    "        \n",
    "        batch = self.memory.sample(batch_size)\n",
    "        for state, action, reward, next_state, terminated in batch:\n",
    "            target = self.policy.model.predict(np.expand_dims(np.array(state), axis=0), verbose = 0)\n",
    "            \n",
    "            # als de state terminal is kunnen we natuurlijk geen future Q value berekenen\n",
    "            if terminated:\n",
    "                target[0][action] = reward\n",
    "            else:\n",
    "                next_action = np.argmax(self.policy.model.predict(np.expand_dims(np.array(next_state), axis=0), verbose=0))\n",
    "                next_q = self.policy.target_model.predict(np.expand_dims(np.array(next_state), axis=0), verbose=0)[0][next_action]\n",
    "                target[0][action] = reward + self.policy.gamma * next_q\n",
    "                \n",
    "            self.policy.model.fit(np.expand_dims(state, axis=0), target, verbose=0)\n",
    "        if self.policy.epsilon > self.policy.min_epsilon:\n",
    "            self.decay()\n",
    "        \n",
    "    def align_target_network(self):\n",
    "        model_weights = self.policy.model.get_weights()\n",
    "        target_model_weights = self.policy.target_model.get_weights()\n",
    "        for i in range(len(target_model_weights)):\n",
    "            target_model_weights[i] = self.policy.tau * model_weights[i] + (1 - self.policy.tau) * target_model_weights[i]\n",
    "        self.policy.target_model.set_weights(target_model_weights)\n",
    "        \n",
    "    def save_model(self, path):\n",
    "        self.policy.model.save(path)\n",
    "\n",
    "        \n",
    "    def load_model(self, path):\n",
    "        self.policy.model = keras.models.load_model(path)\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EpsilonGreedyPolicy:\n",
    "    \n",
    "    def __init__(self,epsilon, epsilon_decay_rate, state_size, action_size, alpha, gamma, min_epsilon, tau):\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay_rate = epsilon_decay_rate\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.min_epsilon = min_epsilon\n",
    "        self.tau = tau\n",
    "        \n",
    "        self.model = self.build_model()\n",
    "        self.target_model = self.build_model()\n",
    "        \n",
    "    def select_action(self, state):\n",
    "        if random.uniform(0,1) < self.epsilon:\n",
    "            action = random.choice(range(self.action_size))\n",
    "        else:\n",
    "            action = np.argmax(self.model.predict(np.expand_dims(np.array(state), axis=0), verbose = 0))\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def build_model(self):\n",
    "        \n",
    "        model = Sequential()\n",
    "        model.add(Dense(32, input_dim=self.state_size, activation='relu'))\n",
    "        model.add(Dense(32, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss='mse', optimizer=Adam(learning_rate=self.alpha))\n",
    "    \n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "agent = Agent(policy = EpsilonGreedyPolicy(epsilon = 1, epsilon_decay_rate = 0.99941, state_size = 8, action_size = 4, alpha = 0.0001, gamma = 0.99, min_epsilon = 0.05, tau = 1), memory = Memory(capacity = 250000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0\n",
      "reward: -87.47672131110915\n",
      "epsilon 1\n",
      "\n",
      "episode: 1\n",
      "reward: -312.69658051532645\n",
      "epsilon 0.99941\n",
      "\n",
      "episode: 2\n",
      "reward: -168.73025914748004\n",
      "epsilon 0.9988203481000001\n",
      "\n",
      "episode: 3\n",
      "reward: -175.3297580717918\n",
      "epsilon 0.9982310440946212\n",
      "\n",
      "episode: 4\n",
      "reward: -128.32086925224488\n",
      "epsilon 0.9976420877786053\n",
      "\n",
      "episode: 5\n",
      "reward: -171.46019473872093\n",
      "epsilon 0.997053478946816\n",
      "\n",
      "episode: 6\n",
      "reward: -118.21458979457884\n",
      "epsilon 0.9964652173942373\n",
      "\n",
      "episode: 7\n",
      "reward: -159.51275627081344\n",
      "epsilon 0.9958773029159748\n",
      "\n",
      "episode: 8\n",
      "reward: -311.9353139055627\n",
      "epsilon 0.9952897353072544\n",
      "\n",
      "episode: 9\n",
      "reward: -182.23306359785192\n",
      "epsilon 0.9947025143634232\n",
      "\n",
      "episode: 10\n",
      "reward: -140.3631542879391\n",
      "epsilon 0.9941156398799488\n",
      "\n",
      "episode: 11\n",
      "reward: -87.7178758601919\n",
      "epsilon 0.9935291116524196\n",
      "\n",
      "episode: 12\n",
      "reward: -339.75461737876947\n",
      "epsilon 0.9929429294765447\n",
      "\n",
      "episode: 13\n",
      "reward: -180.66060686930464\n",
      "epsilon 0.9923570931481536\n",
      "\n",
      "episode: 14\n",
      "reward: -105.3862258713829\n",
      "epsilon 0.9917716024631962\n",
      "\n",
      "episode: 15\n",
      "reward: -315.3167599570534\n",
      "epsilon 0.9911864572177429\n",
      "\n",
      "episode: 16\n",
      "reward: -318.4505964219528\n",
      "epsilon 0.9906016572079844\n",
      "\n",
      "episode: 17\n",
      "reward: -95.92566906797468\n",
      "epsilon 0.9900172022302317\n",
      "\n",
      "episode: 18\n",
      "reward: -88.2768731395465\n",
      "epsilon 0.9894330920809159\n",
      "\n",
      "episode: 19\n",
      "reward: -250.85040740383351\n",
      "epsilon 0.9888493265565882\n",
      "\n",
      "episode: 20\n",
      "reward: -194.0904268162289\n",
      "epsilon 0.9882659054539198\n",
      "\n",
      "episode: 21\n",
      "reward: -36.43693176716129\n",
      "epsilon 0.987682828569702\n",
      "\n",
      "episode: 22\n",
      "reward: -91.00223383931076\n",
      "epsilon 0.9871000957008459\n",
      "\n",
      "episode: 23\n",
      "reward: -122.81946677188624\n",
      "epsilon 0.9865177066443824\n",
      "\n",
      "episode: 24\n",
      "reward: -20.73480790915079\n",
      "epsilon 0.9859356611974622\n",
      "\n",
      "episode: 25\n",
      "reward: -198.59450086862233\n",
      "epsilon 0.9853539591573558\n",
      "\n",
      "episode: 26\n",
      "reward: -36.440997594631966\n",
      "epsilon 0.984772600321453\n",
      "\n",
      "episode: 27\n",
      "reward: -171.54037082269838\n",
      "epsilon 0.9841915844872633\n",
      "\n",
      "episode: 28\n",
      "reward: -375.2123775395009\n",
      "epsilon 0.9836109114524159\n",
      "\n",
      "episode: 29\n",
      "reward: -336.57212186323375\n",
      "epsilon 0.9830305810146589\n",
      "\n",
      "episode: 30\n",
      "reward: -76.28896762666203\n",
      "epsilon 0.9824505929718603\n",
      "\n",
      "episode: 31\n",
      "reward: -281.95059319347274\n",
      "epsilon 0.981870947122007\n",
      "\n",
      "episode: 32\n",
      "reward: -133.44692656128325\n",
      "epsilon 0.981291643263205\n",
      "\n",
      "episode: 33\n",
      "reward: -90.44823356327566\n",
      "epsilon 0.9807126811936797\n",
      "\n",
      "episode: 34\n",
      "reward: -134.10858849599924\n",
      "epsilon 0.9801340607117754\n",
      "\n",
      "episode: 35\n",
      "reward: -103.49462718942347\n",
      "epsilon 0.9795557816159555\n",
      "\n",
      "episode: 36\n",
      "reward: -427.32212585959564\n",
      "epsilon 0.9789778437048021\n",
      "\n",
      "episode: 37\n",
      "reward: -111.81811214953662\n",
      "epsilon 0.9784002467770163\n",
      "\n",
      "episode: 38\n",
      "reward: -91.74032388300759\n",
      "epsilon 0.9778229906314179\n",
      "\n",
      "episode: 39\n",
      "reward: -186.79371079122947\n",
      "epsilon 0.9772460750669454\n",
      "\n",
      "episode: 40\n",
      "reward: -280.8494366017714\n",
      "epsilon 0.976669499882656\n",
      "\n",
      "episode: 41\n",
      "reward: -70.99409335419944\n",
      "epsilon 0.9760932648777252\n",
      "\n",
      "episode: 42\n",
      "reward: -193.38094168451767\n",
      "epsilon 0.9755173698514473\n",
      "\n",
      "episode: 43\n",
      "reward: -74.00382768841962\n",
      "epsilon 0.974941814603235\n",
      "\n",
      "episode: 44\n",
      "reward: -225.50904853744797\n",
      "epsilon 0.9743665989326191\n",
      "\n",
      "episode: 45\n",
      "reward: -242.09802115782549\n",
      "epsilon 0.9737917226392488\n",
      "\n",
      "episode: 46\n",
      "reward: -355.52595167257914\n",
      "epsilon 0.9732171855228917\n",
      "\n",
      "episode: 47\n",
      "reward: -101.58666871697382\n",
      "epsilon 0.9726429873834332\n",
      "\n",
      "episode: 48\n",
      "reward: -125.01834342161557\n",
      "epsilon 0.9720691280208771\n",
      "\n",
      "episode: 49\n",
      "reward: -373.6593366594995\n",
      "epsilon 0.9714956072353448\n",
      "\n",
      "episode: 50\n",
      "reward: -171.3307893765304\n",
      "epsilon 0.9709224248270759\n",
      "\n",
      "episode: 51\n",
      "reward: -186.1125099381202\n",
      "epsilon 0.970349580596428\n",
      "\n",
      "episode: 52\n",
      "reward: -379.4656838081934\n",
      "epsilon 0.969777074343876\n",
      "\n",
      "episode: 53\n",
      "reward: -208.91035120369042\n",
      "epsilon 0.9692049058700132\n",
      "\n",
      "episode: 54\n",
      "reward: -118.95257021228574\n",
      "epsilon 0.9686330749755498\n",
      "\n",
      "episode: 55\n",
      "reward: -377.23770043882763\n",
      "epsilon 0.9680615814613143\n",
      "\n",
      "episode: 56\n",
      "reward: -69.93798558500487\n",
      "epsilon 0.9674904251282521\n",
      "\n",
      "episode: 57\n",
      "reward: -183.15075772788745\n",
      "epsilon 0.9669196057774264\n",
      "\n",
      "episode: 58\n",
      "reward: -361.9727098412609\n",
      "epsilon 0.9663491232100178\n",
      "\n",
      "episode: 59\n",
      "reward: -332.43549751510017\n",
      "epsilon 0.9657789772273239\n",
      "\n",
      "episode: 60\n",
      "reward: -367.18686734709274\n",
      "epsilon 0.9652091676307598\n",
      "\n",
      "episode: 61\n",
      "reward: -79.63513177796099\n",
      "epsilon 0.9646396942218577\n",
      "\n",
      "episode: 62\n",
      "reward: -341.9808027531835\n",
      "epsilon 0.9640705568022668\n",
      "\n",
      "episode: 63\n",
      "reward: -282.7407452763785\n",
      "epsilon 0.9635017551737535\n",
      "\n",
      "episode: 64\n",
      "reward: -284.3942623714002\n",
      "epsilon 0.962933289138201\n",
      "\n",
      "episode: 65\n",
      "reward: -374.2999278316297\n",
      "epsilon 0.9623651584976095\n",
      "\n",
      "episode: 66\n",
      "reward: -272.69688604034667\n",
      "epsilon 0.9617973630540959\n",
      "\n",
      "episode: 67\n",
      "reward: -300.761623203805\n",
      "epsilon 0.961229902609894\n",
      "\n",
      "episode: 68\n",
      "reward: -371.43910378227474\n",
      "epsilon 0.9606627769673542\n",
      "\n",
      "episode: 69\n",
      "reward: -108.33422416177089\n",
      "epsilon 0.9600959859289435\n",
      "\n",
      "episode: 70\n",
      "reward: -162.2159868664188\n",
      "epsilon 0.9595295292972454\n",
      "\n",
      "episode: 71\n",
      "reward: -341.51783538119497\n",
      "epsilon 0.9589634068749601\n",
      "\n",
      "episode: 72\n",
      "reward: -7.84256042805012\n",
      "epsilon 0.9583976184649039\n",
      "\n",
      "episode: 73\n",
      "reward: -131.328335410348\n",
      "epsilon 0.9578321638700096\n",
      "\n",
      "episode: 74\n",
      "reward: -219.5916169165881\n",
      "epsilon 0.9572670428933263\n",
      "\n",
      "episode: 75\n",
      "reward: -167.79438443510222\n",
      "epsilon 0.9567022553380192\n",
      "\n",
      "episode: 76\n",
      "reward: -249.10215846764083\n",
      "epsilon 0.9561378010073698\n",
      "\n",
      "episode: 77\n",
      "reward: -272.51234407044683\n",
      "epsilon 0.9555736797047755\n",
      "\n",
      "episode: 78\n",
      "reward: -63.51370311881571\n",
      "epsilon 0.9550098912337497\n",
      "\n",
      "episode: 79\n",
      "reward: -172.01867338622537\n",
      "epsilon 0.9544464353979218\n",
      "\n",
      "episode: 80\n",
      "reward: -115.56336757960214\n",
      "epsilon 0.953883312001037\n",
      "\n",
      "episode: 81\n",
      "reward: -365.9629945981961\n",
      "epsilon 0.9533205208469564\n",
      "\n",
      "episode: 82\n",
      "reward: 38.483071057679524\n",
      "epsilon 0.9527580617396567\n",
      "\n",
      "episode: 83\n",
      "reward: -90.68427595248043\n",
      "epsilon 0.9521959344832304\n",
      "\n",
      "episode: 84\n",
      "reward: -111.1899173261147\n",
      "epsilon 0.9516341388818853\n",
      "\n",
      "episode: 85\n",
      "reward: -166.54746369760568\n",
      "epsilon 0.9510726747399451\n",
      "\n",
      "episode: 86\n",
      "reward: -333.24097671640254\n",
      "epsilon 0.9505115418618485\n",
      "\n",
      "episode: 87\n",
      "reward: -458.57460722400725\n",
      "epsilon 0.9499507400521501\n",
      "\n",
      "episode: 88\n",
      "reward: -129.97423274544525\n",
      "epsilon 0.9493902691155194\n",
      "\n",
      "episode: 89\n",
      "reward: -88.14556395494904\n",
      "epsilon 0.9488301288567412\n",
      "\n",
      "episode: 90\n",
      "reward: -110.85989391496257\n",
      "epsilon 0.9482703190807158\n",
      "\n",
      "episode: 91\n",
      "reward: -103.9555113271139\n",
      "epsilon 0.9477108395924582\n",
      "\n",
      "episode: 92\n",
      "reward: -30.230079735787044\n",
      "epsilon 0.9471516901970987\n",
      "\n",
      "episode: 93\n",
      "reward: -29.66255495127359\n",
      "epsilon 0.9465928706998824\n",
      "\n",
      "episode: 94\n",
      "reward: -436.5410739402347\n",
      "epsilon 0.9460343809061695\n",
      "\n",
      "episode: 95\n",
      "reward: -89.2161932424907\n",
      "epsilon 0.9454762206214349\n",
      "\n",
      "episode: 96\n",
      "reward: -196.7184859938931\n",
      "epsilon 0.9449183896512683\n",
      "\n",
      "episode: 97\n",
      "reward: -80.13938663827912\n",
      "epsilon 0.9443608878013741\n",
      "\n",
      "episode: 98\n",
      "reward: -353.2364743589731\n",
      "epsilon 0.9438037148775713\n",
      "\n",
      "episode: 99\n",
      "reward: -83.99862079010857\n",
      "epsilon 0.9432468706857936\n",
      "\n",
      "episode: 100\n",
      "reward: -324.5385462251163\n",
      "epsilon 0.942690355032089\n",
      "\n",
      "episode: 101\n",
      "reward: -330.8182459302464\n",
      "epsilon 0.94213416772262\n",
      "\n",
      "episode: 102\n",
      "reward: -124.22297691105497\n",
      "epsilon 0.9415783085636638\n",
      "\n",
      "episode: 103\n",
      "reward: -255.94885076708542\n",
      "epsilon 0.9410227773616112\n",
      "\n",
      "episode: 104\n",
      "reward: -87.396055435434\n",
      "epsilon 0.9404675739229679\n",
      "\n",
      "episode: 105\n",
      "reward: -117.68362484641224\n",
      "epsilon 0.9399126980543533\n",
      "\n",
      "episode: 106\n",
      "reward: -204.68390153262885\n",
      "epsilon 0.9393581495625012\n",
      "\n",
      "episode: 107\n",
      "reward: -311.1923107182771\n",
      "epsilon 0.9388039282542593\n",
      "\n",
      "episode: 108\n",
      "reward: -103.85500159359682\n",
      "epsilon 0.9382500339365893\n",
      "\n",
      "episode: 109\n",
      "reward: -433.28847228629513\n",
      "epsilon 0.9376964664165668\n",
      "\n",
      "episode: 110\n",
      "reward: -107.11268986969914\n",
      "epsilon 0.9371432255013811\n",
      "\n",
      "episode: 111\n",
      "reward: -66.62643541035182\n",
      "epsilon 0.9365903109983352\n",
      "\n",
      "episode: 112\n",
      "reward: -16.591579919753016\n",
      "epsilon 0.9360377227148462\n",
      "\n",
      "episode: 113\n",
      "reward: -357.0537582882421\n",
      "epsilon 0.9354854604584445\n",
      "\n",
      "episode: 114\n",
      "reward: -88.3066844850399\n",
      "epsilon 0.934933524036774\n",
      "\n",
      "episode: 115\n",
      "reward: -73.72696996811018\n",
      "epsilon 0.9343819132575923\n",
      "\n",
      "episode: 116\n",
      "reward: -407.2991955487246\n",
      "epsilon 0.9338306279287704\n",
      "\n",
      "episode: 117\n",
      "reward: -227.79306562280243\n",
      "epsilon 0.9332796678582924\n",
      "\n",
      "episode: 118\n",
      "reward: -339.93300964324203\n",
      "epsilon 0.932729032854256\n",
      "\n",
      "episode: 119\n",
      "reward: -161.55262531939675\n",
      "epsilon 0.932178722724872\n",
      "\n",
      "episode: 120\n",
      "reward: -86.32856028264972\n",
      "epsilon 0.9316287372784644\n",
      "\n",
      "episode: 121\n",
      "reward: -115.26922926713117\n",
      "epsilon 0.9310790763234701\n",
      "\n",
      "episode: 122\n",
      "reward: -171.77514403385692\n",
      "epsilon 0.9305297396684392\n",
      "\n",
      "episode: 123\n",
      "reward: -40.9367283924653\n",
      "epsilon 0.9299807271220348\n",
      "\n",
      "episode: 124\n",
      "reward: -360.54453734405007\n",
      "epsilon 0.9294320384930328\n",
      "\n",
      "episode: 125\n",
      "reward: -88.68732751512805\n",
      "epsilon 0.928883673590322\n",
      "\n",
      "episode: 126\n",
      "reward: -309.3059386500894\n",
      "epsilon 0.9283356322229037\n",
      "\n",
      "episode: 127\n",
      "reward: -378.38094169568507\n",
      "epsilon 0.9277879141998923\n",
      "\n",
      "episode: 128\n",
      "reward: -117.23570241699615\n",
      "epsilon 0.9272405193305143\n",
      "\n",
      "episode: 129\n",
      "reward: -105.65707925764788\n",
      "epsilon 0.9266934474241093\n",
      "\n",
      "episode: 130\n",
      "reward: -283.26248469375685\n",
      "epsilon 0.9261466982901291\n",
      "\n",
      "episode: 131\n",
      "reward: -87.94607734142126\n",
      "epsilon 0.925600271738138\n",
      "\n",
      "episode: 132\n",
      "reward: -123.73754234516761\n",
      "epsilon 0.9250541675778124\n",
      "\n",
      "episode: 133\n",
      "reward: -296.4834958403045\n",
      "epsilon 0.9245083856189416\n",
      "\n",
      "episode: 134\n",
      "reward: -122.01399646243685\n",
      "epsilon 0.9239629256714263\n",
      "\n",
      "episode: 135\n",
      "reward: -112.97336655741731\n",
      "epsilon 0.9234177875452803\n",
      "\n",
      "episode: 136\n",
      "reward: -517.4320195858386\n",
      "epsilon 0.9228729710506286\n",
      "\n",
      "episode: 137\n",
      "reward: -119.44474501578198\n",
      "epsilon 0.9223284759977087\n",
      "\n",
      "episode: 138\n",
      "reward: -31.850046694392134\n",
      "epsilon 0.9217843021968701\n",
      "\n",
      "episode: 139\n",
      "reward: -303.3620340403617\n",
      "epsilon 0.921240449458574\n",
      "\n",
      "episode: 140\n",
      "reward: -227.02985899747944\n",
      "epsilon 0.9206969175933934\n",
      "\n",
      "episode: 141\n",
      "reward: -256.37308513499613\n",
      "epsilon 0.9201537064120133\n",
      "\n",
      "episode: 142\n",
      "reward: -332.6337255339104\n",
      "epsilon 0.9196108157252303\n",
      "\n",
      "episode: 143\n",
      "reward: -124.305357995114\n",
      "epsilon 0.9190682453439524\n",
      "\n",
      "episode: 144\n",
      "reward: -405.8929561461921\n",
      "epsilon 0.9185259950791994\n",
      "\n",
      "episode: 145\n",
      "reward: -51.119447621361346\n",
      "epsilon 0.9179840647421027\n",
      "\n",
      "episode: 146\n",
      "reward: -176.31543387043502\n",
      "epsilon 0.9174424541439049\n",
      "\n",
      "episode: 147\n",
      "reward: -66.1080249618712\n",
      "epsilon 0.91690116309596\n",
      "\n",
      "episode: 148\n",
      "reward: -116.03115743098857\n",
      "epsilon 0.9163601914097335\n",
      "\n",
      "episode: 149\n",
      "reward: -130.70614563303474\n",
      "epsilon 0.9158195388968018\n",
      "\n",
      "episode: 150\n",
      "reward: -312.87477216214376\n",
      "epsilon 0.9152792053688527\n",
      "\n",
      "episode: 151\n",
      "reward: -251.23492876493125\n",
      "epsilon 0.914739190637685\n",
      "\n",
      "episode: 152\n",
      "reward: -130.52387001577313\n",
      "epsilon 0.9141994945152089\n",
      "\n",
      "episode: 153\n",
      "reward: -180.7733536853031\n",
      "epsilon 0.913660116813445\n",
      "\n",
      "episode: 154\n",
      "reward: -158.217244029695\n",
      "epsilon 0.913121057344525\n",
      "\n",
      "episode: 155\n",
      "reward: -128.38922996491218\n",
      "epsilon 0.9125823159206917\n",
      "\n",
      "episode: 156\n",
      "reward: -265.24723056838604\n",
      "epsilon 0.9120438923542985\n",
      "\n",
      "episode: 157\n",
      "reward: -72.97817842571688\n",
      "epsilon 0.9115057864578094\n",
      "\n",
      "episode: 158\n",
      "reward: -443.9949171117639\n",
      "epsilon 0.9109679980437994\n",
      "\n",
      "episode: 159\n",
      "reward: -225.76986138844413\n",
      "epsilon 0.9104305269249535\n",
      "\n",
      "episode: 160\n",
      "reward: -122.49581519965655\n",
      "epsilon 0.9098933729140678\n",
      "\n",
      "episode: 161\n",
      "reward: -75.44289432220783\n",
      "epsilon 0.9093565358240485\n",
      "\n",
      "episode: 162\n",
      "reward: -333.0581624217099\n",
      "epsilon 0.9088200154679124\n",
      "\n",
      "episode: 163\n",
      "reward: -200.90612729520586\n",
      "epsilon 0.9082838116587864\n",
      "\n",
      "episode: 164\n",
      "reward: -97.35980177015158\n",
      "epsilon 0.9077479242099077\n",
      "\n",
      "episode: 165\n",
      "reward: -96.39181026445678\n",
      "epsilon 0.9072123529346239\n",
      "\n",
      "episode: 166\n",
      "reward: -195.54757593321006\n",
      "epsilon 0.9066770976463925\n",
      "\n",
      "episode: 167\n",
      "reward: -405.21666863612063\n",
      "epsilon 0.9061421581587811\n",
      "\n",
      "episode: 168\n",
      "reward: -92.3695739747359\n",
      "epsilon 0.9056075342854674\n",
      "\n",
      "episode: 169\n",
      "reward: -301.04959228687926\n",
      "epsilon 0.905073225840239\n",
      "\n",
      "episode: 170\n",
      "reward: -90.6912219043234\n",
      "epsilon 0.9045392326369932\n",
      "\n",
      "episode: 171\n",
      "reward: -165.06616891606615\n",
      "epsilon 0.9040055544897374\n",
      "\n",
      "episode: 172\n",
      "reward: -380.69224986519885\n",
      "epsilon 0.9034721912125885\n",
      "\n",
      "episode: 173\n",
      "reward: -115.30828254569278\n",
      "epsilon 0.902939142619773\n",
      "\n",
      "episode: 174\n",
      "reward: -298.6984551014059\n",
      "epsilon 0.9024064085256274\n",
      "\n",
      "episode: 175\n",
      "reward: -113.5000283841837\n",
      "epsilon 0.9018739887445972\n",
      "\n",
      "episode: 176\n",
      "reward: -139.41791853524768\n",
      "epsilon 0.901341883091238\n",
      "\n",
      "episode: 177\n",
      "reward: -132.97055782800047\n",
      "epsilon 0.9008100913802142\n",
      "\n",
      "episode: 178\n",
      "reward: -94.31804600015094\n",
      "epsilon 0.9002786134262999\n",
      "\n",
      "episode: 179\n",
      "reward: -102.08855136159458\n",
      "epsilon 0.8997474490443784\n",
      "\n",
      "episode: 180\n",
      "reward: -122.89978252192542\n",
      "epsilon 0.8992165980494422\n",
      "\n",
      "episode: 181\n",
      "reward: -62.725745815869445\n",
      "epsilon 0.898686060256593\n",
      "\n",
      "episode: 182\n",
      "reward: -76.27665928672351\n",
      "epsilon 0.8981558354810416\n",
      "\n",
      "episode: 183\n",
      "reward: -117.42041837340551\n",
      "epsilon 0.8976259235381078\n",
      "\n",
      "episode: 184\n",
      "reward: -161.86278510089733\n",
      "epsilon 0.8970963242432203\n",
      "\n",
      "episode: 185\n",
      "reward: -413.22986402937215\n",
      "epsilon 0.8965670374119168\n",
      "\n",
      "episode: 186\n",
      "reward: -476.7151234036672\n",
      "epsilon 0.8960380628598438\n",
      "\n",
      "episode: 187\n",
      "reward: -86.10268196335329\n",
      "epsilon 0.8955094004027565\n",
      "\n",
      "episode: 188\n",
      "reward: -82.74238891330131\n",
      "epsilon 0.8949810498565189\n",
      "\n",
      "episode: 189\n",
      "reward: -372.4979009419602\n",
      "epsilon 0.8944530110371036\n",
      "\n",
      "episode: 190\n",
      "reward: -132.56699490534976\n",
      "epsilon 0.8939252837605918\n",
      "\n",
      "episode: 191\n",
      "reward: -174.25501332210195\n",
      "epsilon 0.893397867843173\n",
      "\n",
      "episode: 192\n",
      "reward: -54.146506720701424\n",
      "epsilon 0.8928707631011455\n",
      "\n",
      "episode: 193\n",
      "reward: -181.7848793099076\n",
      "epsilon 0.8923439693509159\n",
      "\n",
      "episode: 194\n",
      "reward: -82.88074598867192\n",
      "epsilon 0.8918174864089988\n",
      "\n",
      "episode: 195\n",
      "reward: -315.1439466326086\n",
      "epsilon 0.8912913140920176\n",
      "\n",
      "episode: 196\n",
      "reward: -162.44026601432446\n",
      "epsilon 0.8907654522167033\n",
      "\n",
      "episode: 197\n",
      "reward: -124.2950138765635\n",
      "epsilon 0.8902399005998956\n",
      "\n",
      "episode: 198\n",
      "reward: -136.2880601447594\n",
      "epsilon 0.8897146590585416\n",
      "\n",
      "episode: 199\n",
      "reward: -77.75711530604298\n",
      "epsilon 0.8891897274096972\n",
      "\n",
      "episode: 200\n",
      "reward: -306.49532860316947\n",
      "epsilon 0.8886651054705255\n",
      "\n",
      "episode: 201\n",
      "reward: -101.68933330938775\n",
      "epsilon 0.8881407930582979\n",
      "\n",
      "episode: 202\n",
      "reward: -146.3341118087007\n",
      "epsilon 0.8876167899903935\n",
      "\n",
      "episode: 203\n",
      "reward: -77.13679830002602\n",
      "epsilon 0.8870930960842992\n",
      "\n",
      "episode: 204\n",
      "reward: -266.40478316333554\n",
      "epsilon 0.8865697111576095\n",
      "\n",
      "episode: 205\n",
      "reward: -80.59679519911583\n",
      "epsilon 0.8860466350280265\n",
      "\n",
      "episode: 206\n",
      "reward: -185.11222269408938\n",
      "epsilon 0.88552386751336\n",
      "\n",
      "episode: 207\n",
      "reward: -232.90967495974172\n",
      "epsilon 0.8850014084315271\n",
      "\n",
      "episode: 208\n",
      "reward: -396.0320807791426\n",
      "epsilon 0.8844792576005525\n",
      "\n",
      "episode: 209\n",
      "reward: -82.77508877314662\n",
      "epsilon 0.8839574148385682\n",
      "\n",
      "episode: 210\n",
      "reward: -129.34280694767557\n",
      "epsilon 0.8834358799638135\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"LunarLander-v2\", render_mode=\"rgb_array\")\n",
    "\n",
    "episodes = 5000\n",
    "for episode in range(episodes):\n",
    "    \n",
    "    state, info = env.reset()\n",
    "    ep_reward = 0\n",
    "    while True:\n",
    "        action = agent.select_action(state)\n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "        ep_reward += reward\n",
    "        agent.memory.store(Transition(state, action, reward, next_state, terminated))\n",
    "        state = next_state\n",
    "\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "    print(f\"episode: {episode}\\nreward: {ep_reward}\\nepsilon {agent.policy.epsilon}\\n\")\n",
    "    agent.train(32)\n",
    "    agent.align_target_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "agent.save_model(\"model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
